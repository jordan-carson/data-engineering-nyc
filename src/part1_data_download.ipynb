{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Data Download\n",
    "\n",
    "To make an API call using python, we use the `requests` library. Before we start coding, we need to read the documentation to understand if we need to set-up any configuration beforehand.\n",
    "\n",
    "Below is a copy+paste of the MVCC [API Documentation](https://dev.socrata.com/foundry/data.cityofnewyork.us/h9gi-nx95), between the START and END headers.\n",
    "#### START\n",
    "\n",
    "### API Documentation \n",
    "\n",
    "#### Getting Started\n",
    "All communication with the API is done through HTTPS, and errors are communicated through HTTP response codes. Available response types include JSON (including GeoJSON), XML, and CSV, which are selectable by the \"extension\" (.json, etc.) on the API endpoint or through content-negotiation with HTTP Accepts headers.\n",
    "\n",
    "This documentation also includes inline, runable examples. Click on any link that contains a  gear symbol next to it to run that example live against the Motor Vehicle Collisions - Crashes API. If you just want to grab the API endpoint and go, you'll find it below.\n",
    "\n",
    "#### Tokens\n",
    "All requests should include an app token that identifies your application, and each application should have its own unique app token. A limited number of requests can be made without an app token, but they are subject to much lower throttling limits than request that do include one. With an app token, your application is guaranteed access to it's own pool of requests. If you don't have an app token yet, click the button to the right to sign up for one.\n",
    "\n",
    "Once you have an app token, you can include it with your request either by using the X-App-Token HTTP header, or by passing it via the $$app_token parameter on your URL.\n",
    "\n",
    "#### END\n",
    "\n",
    "**The above tells us a few important points:**\n",
    "1. All API calls are done via HTTPS and errors are communicated through HTTP response codes. Response codes indicate to the client what has happened. Typically a response of 200/201 indicates a success, while a 401/403 indicates an error.\n",
    "\n",
    "2. We need to use {'X-App-Token': APP_TOKEN} as our headers to pass to our GET request. Even though this is mentioned, there is no mention on where we need to pass our SECRET. Thus, I suspect this header is an optional argument.\n",
    "\n",
    "### Things to Know / Plans\n",
    "\n",
    "We have multiple methods to query the underlying API. \n",
    "We can use a basic cURL request, the NYC OpenData [Socrata API](https://dev.socrata.com/docs/queries/), and/or write a SQL query to pull from the google [bigquery-public-data project](https://cloud.google.com/bigquery/public-data). \n",
    "\n",
    "1. The API Documentation from the Socrate API above tells us that we need to use [Paging Through Data](https://dev.socrata.com/docs/paging.html) to pull all the 1.8 million records from the table. This is because the API defaults the limit to 1000 records returned. Paging through the data allows us to set an offset index, which tells the API where to start the returned list of results. It is important to mention that the data has to be ordered properly to ensure the results will be stable as we page through the dataset.\n",
    "2. The dataset has 1.83 Million Rows.\n",
    "3. There are several noticable data quality issues which we will discuss later.\n",
    "4. After we have created the simple API, we are going to parallelize the IO, increase the number of workers to assist with getting this data more rapidly.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: sys.path and Imports\n",
    "\n",
    "Best practice to include the cwd parent within our sys.path. Next, we include the relevant imports for querying the API. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "CWD = str(Path.cwd().parent)\n",
    "sys.path.append(CWD)\n",
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import base64 \n",
    "import traceback\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core import api\n",
    "from sodapy import Socrata # Alternative \n",
    "\n",
    "from tqdm import tqdm\n",
    "# personal common library\n",
    "from common.utilities import decorators \n",
    "from secrets_manager import get_secret\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(CWD)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/jordancarson/Projects/JPM/data-engineering-nyc\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We keep all secrets and application tokens in AWS Secrets Manager also known as Key Management Service (KMS). The below json will **not** be published to git. [Link to AWS KMS](https://us-east-1.console.aws.amazon.com/kms/home?region=us-east-1#/kms/keys/000d47a4-e093-4ffc-8b99-8f6f71381fcc)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "secretARN = json.load(open(os.path.join(CWD, 'secret.json')))[0]\n",
    "SECRET_DATA = json.loads(get_secret(os.getenv('AWS_SECRET_NAME_NYC_OPEN_DATA') or secretARN.get('AWS_SECRET_NAME_NYC_OPEN_DATA')))\n",
    "assert isinstance(SECRET_DATA, dict), 'SECRET_DATA was not loaded properly!'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# API Constants\n",
    "API_LIMIT = 50000 # we want to pull 50,000 records at each iteration\n",
    "NYC_OPEN_DATA_API_ENDPOINT = 'https://data.cityofnewyork.us/resource/h9gi-nx95.json'\n",
    "NYC_OPEN_DATA_API_KEY = SECRET_DATA[\"NYC_OPEN_DATA_API_KEY\"]\n",
    "NYC_OPEN_DATA_API_SECRET =  SECRET_DATA['NYC_OPEN_DATA_API_SECRET']\n",
    "NYC_OPEN_DATA_APP_TOKEN = SECRET_DATA['NYC_OPEN_DATA_APP_TOKEN']\n",
    "NYC_OPEN_DATA_APP_SECRET = SECRET_DATA['NYC_OPEN_DATA_APP_SECRET']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Query the Data\n",
    "\n",
    "Next, we create a function to call the API using the offset and limit parameters in our request URL. The below code snippet downloads all the data and returns a single Pandas Dataframe. The problem with this implementation is that it takes **a lot of time** to query all the data. \n",
    "\n",
    "We are using the [paging](https://dev.socrata.com/docs/paging.html) option, which allows us to set `offset` and `limit` parameters. To query all the data we need to  we need to create a HTML based SOAP query. To do this we add parameters to the back of the ENDPOINT. \n",
    "\n",
    "**Iteration 0**\n",
    "```python\n",
    "API_LIMIT   = 50000\n",
    "OFFSET      = 0\n",
    "ID          = 'collision_id'\n",
    "ENDPOINT    = f'https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit={API_LIMIT}&$offset={offset}&$order={ID}'\n",
    "```\n",
    "\n",
    "**Iteration >0**\n",
    "```python\n",
    "API_LIMIT   = 50000\n",
    "OFFSET     += API_LIMIT or pd.read_json(response.text).shape[0] # the number of records will be the same as the API_LIMIT until we reach the last page\n",
    "ID          = 'collision_id'\n",
    "ENDPOINT    = f'https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit={API_LIMIT}&$offset={offset}&$order={ID}'\n",
    "```\n",
    "\n",
    "After the first iteration, the $OFFSET$ parameter will be updated to the value of the API_LIMIT. We will continue to increment the offset parameter by the API_LIMIT until we receive all the data, or until the number of records returned is lower than the limit. This logic can be summarized in the function call below. After the response has been made, we push the `response.text` into a temp dataframe, which will be appended to a list. We then concatenate the list of dataframes outside of the loop. The reason we do not want to concatenate DataFrames within a loop, can be summarized [here](https://stackoverflow.com/questions/36489576/why-does-concatenation-of-dataframes-get-exponentially-slower)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "@decorators.timeit\n",
    "def api_pagination_results(orient = 'records'):\n",
    "    \"\"\"\n",
    "    One method to pull data from the Open Source API is to \n",
    "    \"\"\"\n",
    "    ID = 'collision_id'\n",
    "    finished = False\n",
    "    offset = 0\n",
    "    out_frames = list()\n",
    "    while not finished:\n",
    "        ENDPOINT = f'https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit={API_LIMIT}&$offset={offset}&$order={ID}'\n",
    "        response = requests.get(ENDPOINT, auth=HTTPBasicAuth(NYC_OPEN_DATA_API_KEY, NYC_OPEN_DATA_API_SECRET))\n",
    "\n",
    "        temp_df = pd.read_json(response.text, orient=orient)\n",
    "        length = len(temp_df)\n",
    "        out_frames.append(temp_df)\n",
    "        \n",
    "        offset += temp_df.shape[0] # len(temp_df)\n",
    "\n",
    "        if length < API_LIMIT:\n",
    "            finished = True\n",
    "        \n",
    "    # concatenate the list into one master dataframe, do not do this inside the loop!\n",
    "    df = pd.concat(out_frames, ignore_index=True)\n",
    "    del out_frames, length, offset\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "data = api_pagination_results()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "api_pagination_results => 1101231.782913208 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pagination Results\n",
    "\n",
    "After 1101231.78 ms, all of the data has been returned from the function `api_pagination_results()`. Converting this figure to minutes, we can see that the total time takes ~18 minutes.\n",
    "```\n",
    "print(1101231.782913208 / 1000 / 60)\n",
    ">> 18.353863048553468\n",
    "```\n",
    "### Why does this take so long?\n",
    "A call over a network is IO bound to the caller. There are typically 2 types of performance bottlenecks for most programs, IO or CPU. Python, a dynamic language, has a Global Interpreter Lock (GIL), embedded within, allowing only one thread to hold the control of the python interpreter. Sometimes it makes sense to use multi-threading with IO bound tasks. Using the `concurrent.futures` library we can create a `ThreadPoolExecutor` to increase the number of threads, or execution units, to our program. \n",
    "\n",
    "Another option to speed up our program would be to use `asynchronous programming`. There are two libraries `asyncio` and `aiohttp` that allow you to execute some tasks in a *seemingly concurrent* manner. It is commonly used in web-servers and database connections, and also useful to speed up IO bound tasks. \n",
    "\n",
    "In our next example, we will address the performance bottleneck by using the `concurrent` library. A few new functions will be created.\n",
    "\n",
    "Lets see what this will look like."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def get_one(url):\n",
    "    \"\"\"\n",
    "    Function to make one GET request to the URL specified.\n",
    "    Returns:\n",
    "        DataFrame / response.text\n",
    "    \"\"\"\n",
    "    response = requests.request(\"GET\", url, auth=HTTPBasicAuth(NYC_OPEN_DATA_API_KEY, NYC_OPEN_DATA_API_SECRET))\n",
    "    # return pd.read_json(response.text, orient='records')\n",
    "    return response.text.encode(\"utf8\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now going to parallelize the requests using the ThreadPoolExecutor from the `concurrency` library. The idea is that we will create a function to pull the response from the endpoint, and then use the exector to map the response function to a list of URLs. In this instance, we will need the full-list of URLs ahead of time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "@decorators.timeit\n",
    "def get_all(urls, workers=15):\n",
    "    # https://gist.github.com/rednafi/3334a9cce2d7f24226f6fe1231b5ac5f\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        results = list(\n",
    "            tqdm(executor.map(get_one, urls, timeout=60), total=len(urls))\n",
    "        )\n",
    "        return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create a function to pull all the URLs. In this instance, we need to know ahead of time the entire list of URLs. We can build the different endpoints using the offset and limit parameters in the API. However, there is a drawback to this solution. See the output section below for more detail."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def create_urls(id='collision_id'):\n",
    "    # we start with an offset of 0, we then increment the offset to be equal to the number of records returned. We are specifying the \n",
    "    # number of records returned via the API_LIMIT. \n",
    "    offset, limit = 0, 50000\n",
    "    urls = list()\n",
    "    for _ in range(0, 2_000_000, limit):\n",
    "        ENDPOINT = f'https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit={limit}&$offset={offset}&$order={id}'\n",
    "        urls.append(ENDPOINT)\n",
    "        offset += limit\n",
    "    return urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, we create our `parallel_pagination_api` function. Inside, we have the list of urls from `create_urls()` and the text_results from `get_all()`. The latter returns a generator, which we loop through to get the results. We do this via a list-comprehension, converting the response.text to a dataframe, and then concatenate all the dataframes afterwards."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# @decorators.parallel_task\n",
    "def parallel_pagination_api():\n",
    "    urls = create_urls()\n",
    "    text_results = get_all(urls)\n",
    "    # for result in text_results:\n",
    "    #         final.append(result)\n",
    "    dfs = [pd.read_json(response, orient='records') for response in text_results]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_parallel = parallel_pagination_api()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "get_all => 58675.67992210388 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output\n",
    "\n",
    "Increasing the number of threads speeds up our retreival! The sequential program takes ~18 minutes, while our parallelized algorithm takes ~1-3 minutes. The function returns the data from the API in 1 minute, however it takes another 2 minutes to convert to a pandas dataframe. This is a fantastic improvement, however we have to hard-code a critically important number.\n",
    "```\n",
    "print(58675.67992210388 / 1000 / 60)\n",
    ">> 0.9779279987017314\n",
    "```\n",
    "### Issues with Parallelization for Pagination\n",
    "\n",
    "This solution **requires us** to specify the amount of records - i.e. the max range in `create_urls()`. The NYC OpenData API does not provide a programatic way to locate the the next page, offset, or total records in the MVCC table. Most of the time APIs provide this in the response.headers, however this is not available. **This is one downside to using this approach.**\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "- Given the issue above; it might be worth looking into asynchronous programming.\n",
    "- Lastly, this data is also available within the bigquery table - `bigquery-public-data.new_york_mv_collisions.nypd_mv_collisions`. During my research of this project, I discovered that the number of records increased from 1829000 to 1830092. I immediately queried the table in GCP, and noticed that the data was already updated. This could be the solution to remove the bottleneck on the API side. \n",
    "\n",
    "# Output\n",
    "\n",
    "We are pushing the output to a parquet file, compressed using gzip, to be consumed by our next notebook.\n",
    "\n",
    "Cick Here to access the next workbook!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "try: \n",
    "    df_parallel.to_parquet('output.parquet', compression='gzip')\n",
    "except Exception as err:\n",
    "    print(f'Error occured writing parquet, using CSV. Error: {err}')\n",
    "    df_parallel.to_csv('output.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bigquery example\n",
    "\n",
    "The below query pulls the total number of records from the public dataset `bigquery-public-data.new_york_mv_collisions.nypd_mv_collisions`. All columns are the same with the exception of collision_id, this is called unique_key in the below table."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from bigquery import get_client\n",
    "from time import sleep\n",
    "SQL = \"\"\"\n",
    "#standardSQL\n",
    "SELECT COUNT(distinct unique_key) FROM `bigquery-public-data.new_york_mv_collisions.nypd_mv_collisions` a\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('3.9.5': pyenv)"
  },
  "interpreter": {
   "hash": "fdd338dfa850261f035b26e14a7286b8675656f858c7969cf6cbcb21f8762763"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}